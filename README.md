LLMentor
========

Ovvero perché è importante fornire gli insegnamenti migliori ai modelli del futuro
----------------------------------------------------------------------------------

Manifesto per una piattaforma dove consentire la pubblicazione da parte di docenti universitari, ricercatori, dottorati di conversazioni con i Chatbot basati su Large Language Models

La diffusione al grande pubblico di ChatGPT ha fatto si che i progressi ottenuti dai *Large Language Models* (di seguito, "modelli" o LLMs) nella generazione di testi in linguaggio naturale abbiano ottenuto una grande visibilità, facendo così prevedere un imminente impatto di portata più o meno vasta di questi modelli nella vita quotidiana, nel lavoro, nello studio e in generale nella sfera pubblica. I LLMs vengono addestrati su grandi quantitativi di dati, chiamati *dataset* o *corpora* e, attraverso l'analisi di questi dati, i modelli imparano tanto a comprendere quanto a generare testo nelle lingue presenti nel *corpus*; è sempre da questi testi che i modelli estraggono le informazioni che forniranno una volta interrogati.

L'applicazione più nota al momento dei LLMs è certamente il chatbot, come ChatGPT o Bard, ma il loro impiego si estende anche in altri ambiti, come la traduzione automatica o il riassunto di documenti, ed è probabile che in un futuro prossimo nuove idee per utilizzare questi potenti modelli verranno diffuse e realizzate, vista soprattutto la relativa semplicità applicativa.

Come detto, i LLMs hanno bisogno di questi dati e pertanto la loro qualità è cruciale per ottenere applicazioni allineate alle esigenze degli sviluppatori. Gran parte dei dati utilizzati attualmente per addestrare questi modelli derivano da operazioni di "setacciamento" dell'intera rete Internet, un metodo che se da una parte consente di ottenere la enorme quantità di dati testuali richiesta per far emergere le caratteristiche più interessanti dei sistemi conversazionali come i chatbot, dall'altra comporta insidie dovute alla scarsa qualità di molti dei contenuti testuali presenti in rete. Nell'ottica dello sviluppo di LLM open-source, ai fini specialmente di ricerca accademica o nell'ambito di progetti pubblici, è di importanza essenziale costruire *corpora* di qualità elevatissima con cui eseguire la sintonia fine (*fine tuning*) di questi modelli onde evitare risposte meramente scorrette oppure viranti acriticamente verso alcune tendenze .

La necessità di migliorare le prestazioni dei modelli non è limitata allo sviluppo fine a se stesso di questi per fini commerciali o di ricerca: bisogna infatti inquadrare questa esigenza in una possibile questione sociale emergente. Ci sono ottime possibilità che i modelli, a prescindere da quanto una singola persona li possa guardare con favore o meno, diventino sempre più popolari e importanti in tutta la sfera informativa: da assistenti personali a sistemi per completare documenti in modo automatico, passando da strumenti didattici per l'educazione di massa alla stesura di articoli di informazione o contenuti sulle reti sociali; questi sono solo pochi esempi dei vari campi che probabilmente subiranno un impatto massiccio dalla diffusione dei LLM.

Tenendo a mente l'idea che questi modelli hanno un forte potenziale di *modificare* il discorso pubblico in generale, essendo questo veicolato principalmente tramite il linguaggio e avendo i modelli capacità di operare proprio con il linguaggio, è di cruciale importanza *sociale* mantenere un controllo sulla qualità dei dati che sostengono tali modelli ed evitare il rischio di lasciare le decisioni su tali dati esclusivamente a un numero ristretto di attori, spesso di estrazione tecnico-informatica e di nazionalità statunitense; tale rischio potrebbe causare una deviazione, subdola e difficile da osservare, del discorso pubblico e delle opinioni diffuse nella massa verso un singolo centro di gravità concettuale minando così il pluralismo e abbassando pericolosamente la qualità dell'informazione, deviazione probabilmente non voluta nemmeno dagli attori citati ma considerabile un semplice sottoprodotto del mancato controllo sui dati, È per questo che un progetto come LLMentor si rivolge non solo agli "entusiasti" dell'Intelligenza Artificiale Generativa ma anche agli "scettici", ai "contrari", i quali vengono invitati a contribuire allo sviluppo di LLMs liberi al fine di mitigare questi rischi sociali.

**La situazione**

Osservando i risultati forniti da Chatbot come ChatGPT o Bard, il primo aspetto critico che si riscontra è l'eccessiva rilevanza che assume la lingua Inglese rispetto alle altre lingue parlate dall'umanità. Anche osservando i dataset di allenamento di alcuni LLM liberi, come Llama o RedPajama, si nota come l'Inglese abbia il primato assoluto per quanto riguarda i dati inseriti nei modelli, fattore che causa necessariamente una carenza per quanto riguarda l'inclusività di tutti gli altri numerosi e importanti gruppi linguistici. Nonostante le prestazioni di *traduzione* sembrano buone e a una analisi superficiale sufficienti a mitigare questa discrepanza, in realtà il fatto che i dati di addestramento delle LLM siano per la grande maggioranza in inglese continua ad essere problematico. Questo significa infatti una sclerotizzazione sulla cultura anglo-americana, che viene presa automaticamente dai modelli come punto di riferimento rispetto al quale calcolare poi le differenze rispetto a tutte le altre culture. Se in conversazioni su argomenti tecnici o scientifici questa discrepanza non causa problemi eccessivi, è nelle tematiche culturali, sociali e politiche che avere una maggiore rappresentanza di altre lingue e culture nei *corpora* assume rilevanza fondamentale. Lingue diverse ritagliano diversamente i concetti e pertanto il modo migliore per ottenere risultati più coerenti con lingue diverse dall'Inglese consiste nell'includere nei *corpora* grandi quantità di testi provenienti da lingue diverse, avendo come doppio risultato positivo da un lato il miglioramento delle prestazioni nell'uso di altre lingue, tanto sul lato grammaticale quanto principalmente semantico, dall'altro l'inclusione di sfaccettature e opinioni caratteristiche di culture diverse da quella anglo-americana, a vantaggio del pluralismo che un buon modello Open-Source dovrebbe possedere. LLMentor affronta questo problema invitando i contribuenti ad utilizzare preferibilmente la propria lingua, anche e soprattutto se diversa dall'Inglese.

Il secondo punto critico che LLMentor vorrebbe mitigare è relativo alla qualità dei testi: l'utilizzo di setacciamenti massivi del Web come fonte di informazione è spesso causa della generazione e diffusione di informazioni scorrette o banali. Prendendo come riferimento ChatGPT, si nota come i metodi di sintonia fine utilizzati dalla squadra di sviluppo di OpenAi riescano nella maggior parte dei casi a migliorare nettamente questo aspetto tuttavia, allo stesso tempo, introducono artefatti nella conversazione che vengono percepiti come un atteggiamento esageratamente protettivo rispetto a temi e toni della conversazione: accondiscendente con l'utente, piatto, caratterizzato da un politicamente corretto quasi ossessivo e che da per scontato di trovarsi di fronte in ogni caso ad interlocutori privi di spirito critico e da proteggere da qualsiasi idea minimamente "pericolosa". Forse adatto ad un pubblico generalista, questo modo di regolare i modelli rischia di essere particolarmente limitante in ambiti quali quelli accademici e di ricerca. LLMentor, in virtù dell'autorità delle fonti autorizzate a pubblicare, fornisce un metodo per costruire un *corpus* sicuramente ristretto di diversi ordini di grandezza nelle sue dimensioni rispetto a quelli ricavati da Internet ma di qualità ineccepibile e di conseguenza ottimo per eseguire la sintonia fine di modelli open-source, con dati di alta qualità e plurali dal punto di vista tanto linguistico quanto delle opinioni.

Un terzo aspetto su cui l'opera di LLMentor potrebbe dare un contributo è quello dell'attuale difficoltà dei LLM di avere a che fare con contenuti particolarmente specifici e approfonditi; le risposte fornite dai modelli tradiscono spesso una maggiore incertezza quando si discute di questioni per le quali vi sono meno dati disponibili; LLMentor potrebbe aiutare a sviluppare modelli capaci di muoversi meglio in discussioni di livello accademico. Questo non sarebbe utile solamente allo scopo di ottenere modelli più interessanti ma anche, tornando al tema del discorso pubblico, di evitare che questo vada ad appiattirsi su risposte più facili, generiche e foriere quindi di conoscenze piatte e meno aperte al dubbio.

Il quarto aspetto, considerabile il più importante, è quello di cercare di mitigare alcune *tendenze* dei modelli attuali. In primo luogo, si notano spesso prestazioni migliori in ambito tecnico, soprattutto informatico e scientifico rispetto ad ambiti umanistici. Ciò è dovuto a una maggiore quantità e cura di dati provenienti da articoli tecnici e scientifici rispetto a materiale proveniente da altre discipline. Questo sbilanciamento dell'informazione verso l'ambito scientifico può risultare dannosa in caso di una diffusione di massa dei modelli: sebbene i risultati scientifici siano spesso i temi meno problematici su cui discutere, rispetto a questioni sicuramente più delicate come la storia o la politica, avere dei modelli sbilanciati verso questo ambito è dannoso perché comporta il rischio di appiattire la cultura in direzione della scienza, andando ulteriormente ad accentuare una tendenza della società contemporanea a un certo riduzionismo scientista di tematiche più ampie e complesse. Questo ultimo punto, poi, tocca un altro tema che possiamo confidenzialmente chiamare "effetto Wikipedia". Wikipedia, in effetti, è spesso il *corpus* principe che viene utilizzato per allenare i modelli. Il problema di Wikipedia è che, trattandosi di una enciclopedia collaborativa, deve necessariamente presentare descrizioni che siano il più "oggettive" e "neutrali" possibili. Tale neutralità, tuttavia, può funzionare nel caso del discorso scientifico ma rischia di risultare appiattente in tutte le altre tematiche. Il rischio, molto sottile, è di confondere "neutralità" con "verità" e la considerazione da fare è che dei dati eccessivamente "neutrali" rischiano di addestrare modelli che siano "non-neutrali" nel loro essere esageratamente "neutrali". La frase precedente, che sembra un gioco di parole, vuole significare che ci sono tematiche, come la critica artistica, l'analisi storico-politica, la filosofia, la sociologia solo per citarne alcune in cui è necessario non solo essere "neutrali" a tutti i costi ma anche includere opinioni divergenti purché argomentate con cura e sostenute in modo convincente. La neutralità, spesso, è semplicemente irraggiungibile in tali tematiche e l'illusione della sua presenza nasconde in realtà l'aderenza a un certo punto di vista dominante che viene così avallato surrettiziamente e confuso con una impossibile oggettività. Si tratta di uno tra i rischi maggiori relativo alla diffusione dei modelli al grande pubblico: coscienze meno abituate al pensiero critico potrebbero confondere le risposte generiche e pronunciate con tono convincente dei modelli con punti di vista veri, oggettivi e giusti, danneggiando così il pensiero critico e il rispetto di opinioni divergenti. Facile immaginare quanto ciò possa essere rischioso, ad esempio, nel campo della didattica. Allo stesso modo, l'assoluta mancanza di contenuto "emotivo" ed "estetico" da corpus neutrali può significare una grave mancanza per i LLMs, sia dal punto di vista del loro funzionamento efficace in certe conversazioni, sia per il rischio di epurazione di questi contenuti da discorsi influenzati dall'uso di tali modelli. La "neutralità" porta con se il rischio di "appiattimento" su posizioni allineate con lo *status quo* del discorso pubblico contemporaneo dei vari Paesi mentre accogliere opinioni divergenti è fondamentale per tutelare quel pluralismo che è obiettivo cardine di LLMentor. Un meccanismo come quello con cui LLMentor sceglie i contribuenti permette invece di accogliere testi che possono anche allontanarsi da quell'idea di neutralità e sostenere quindi opinioni divergenti rispetto a quelle sostenute dall'informazione di massa, nonché contribuire alla completezza dei modelli con contenuti connotati emotivamente, ad esempio nella critica di un film o di un'opera letteraria. Questo è tra i punti più importanti a sostegno dell'importanza di un progetto come LLMentor perché, in uno scenario distopico in cui le persone facciano affidamento quasi ceco sui risultati dei LLM, ad essere ancora più distopico sarebbe proprio questo appiattimento su discorsi privi di contenuto emotivo, che screditano implicitamente tutto ciò che non sia una verità scientifica valida secondo il metodo galileiano e che, in una illusione di "oggettività", facciano convergere tutte le opinioni verso quelle maggiormente sostenute dal gruppo sociale detentore del potere.

### L'idea

L'idea di LLMentor è quella di costruire una piattaforma per raccogliere testi che abbiano come fine principale quello di addestrare futuri modelli di Intelligenza Artificiale. Dal momento che la caratteristica che differenzia il *corpus* di LLMentor da quello di altri *corpus* generali è la qualità e diversità di prospettive, si è optato per consentire l'accesso in scrittura esclusivamente ad autori appartenenti al panorama accademico. Se da un lato si potrebbe obiettare che si tratta di un approccio "elitario", in realtà ci si rende conto facilmente che sono già numerose le piattaforme dove qualsiasi categoria di utenti può pubblicare le proprie opinioni: forum di discussione, reti sociali, *mailing list* vengono spesso già incluse nei dati di allenamento dei modelli. Quello che manca è proprio una piattaforma dove venga effettuata una selezione delle persone autorizzate a condividere testi. Questa limitazione dell'accesso non sta a significare che gli accademici vengono considerati gli unici detentori del sapere e gli unici personaggi autorizzati ad avere opinioni: questo filtraggio permette di avere un corpus ristretto sul quale si possa avere una garanzia ben maggiore per quanto riguarda autorevolezza e precisione rispetto a quelli ottenuti genericamente da Internet. Non ci si può aspettare che LLMentor diventi l'unica fonte per i modelli linguistici, ma semplicemente uno dei tanti tasselli di un *dataset* ben formato, nel quale includere un numero anche molto ristretto di contenuti etichettati come di qualità può permettere netti miglioramenti. Sono almeno tre i punti di forza di questo approccio: per prima cosa ci si può aspettare un'ottima qualità e livello di approfondimento dei testi per quanto riguarda le discipline studiate dai contribuenti, consentendo così un miglioramento dei dati anche per quanto riguarda aspetti specifici, complessi e per i quali le informazioni presenti nella rete generalista possono essere scarni, banali o addirittura errati; in secondo luogo, come detto uno tra gli obiettivi principali di LLMentor è quello di includere anche opinioni che si distaccano da quelle dominanti nel dibattito contemporaneo e spesso nascoste sotto il pericoloso concetto di "neutralità": da parte di un pubblico accademico ci si aspetta maggiormente che tali opinioni vengano ben argomentate, esposte in un linguaggio corretto e si evita allo stesso tempo la diffusione di idee prive di alcun tipo di fondamento che, sebbene popolari in ambienti quali forum e social, è particolarmente difficile (sebbene non impossibile) riscontrare in una platea accademica; in terzo luogo, essendo le istituzioni universitarie presenti in tutto il mondo, viene risolto il problema di ottenere contenuti di qualità anche in lingue meno rappresentate.

### La tipologia di testi

La categoria principale di testi che vengono raccolti da LLMentor sono le conversazioni con gli stessi chatbot, ad esempio ChatGPT, Google Bard, Poe, Bing e i vari modelli opensource già disponibili. Nonostante sia possibile caricare all'interno della propria pagina personale anche testi liberi (ad esempio, riflessioni, dispense e altro materiale che si desidera rendere liberamente accessibile), l'obiettivo principale è quello di creare un *corpus* basato sulle conversazioni con i Chatbot. Questo è infatti un ottimo modo per allenare altri modelli: i modelli in allenamento hanno così a disposizione esempi di qualità dove vengono messe in evidenza tanto conversazioni costruttive e di ottima qualità quanto conversazioni errate o contraddittorie. Inoltre, costruire un tale *corpus* significa ottenere una risorsa importantissima per lo studio dei modelli stessi: è possibile applicare metodi di informatica umanistica per studiare aspetti quali la loro evoluzione nel tempo, lo stile delle risposte, il lessico utilizzato, le tendenze più comuni (come i bias ideologici impliciti nei modelli) e così via.

Coloro che desiderano partecipare sono invitati a prendere prima un po' di dimestichezza con i chatbot, in modo da comprendere quali siano i meccanismi per ottenere risposte più interessanti; si tratta di sistemi molto intuitivi e pertanto anche pochi minuti sono spesso sufficienti per capire come intraprendere conversazioni costruttive. È importante che le conversazioni non si limitino solo al tema dell'intelligenza artificiale ma al contrario che si espandano ai vari campi di studio dei contribuenti, in modo da raccogliere conversazioni anche particolarmente specifiche e tecniche. La qualità del *prompt* determina completamente il tenore delle risposte: a domande generali corrisponderanno risposte banali e poco utili mentre la conversazione si farà interessante se si riesce a stimolare il modello a riflessioni complesse, anche partendo *in medias res* su un argomento particolarmente complesso. Le conversazioni maggiormente utili saranno quelle in cui si stimolerà il modello ad affrontare tematiche difficili con argomentazioni di elevata qualità, conversazioni nelle quali sarà importante evidenziare tanto le buone risposte dei chatbot ma soprattutto dibattere con loro, presentare obiezioni, contraddire le loro risposte, testare la tenuta del discorso proprio come si farebbe in una discussione con un collega o uno studente. L'idea è che gli accademici possano, anche divertendosi, stimolare discussioni molto elevate portando al limite le capacità dei chatbot ed utilizzando strumenti retorici sottili, rimanendo sempre pronti a mettere in luce eventuali errori concettuali o di ragionamento presenti nelle risposte. Se l'aderenza sarà buona e il risultato rispetterà le aspettative, quella che si otterrà non sarà solo una banca dati fondamentale per l'addestramento e studio dei modelli ma anche una interessante risorsa da leggere e consultare, nonché potrebbe fornire numerosi stimoli e idee agli stessi accademici che decidano di collaborare.

**Contenuti tagliati...**

Un modello come quello di Wikipedia è ottimo per nozioni enciclopediche, a maggior ragione se di contenuto scientifico, ma non è sufficiente per addestrare modelli capaci di ottenere una rilevanza importante in tutti quei modi digitali dell'uso del linguaggio, che hanno un grande impatto in tutti i tipi di discorso pubblico. Il rischio tuttavia di allenare i modelli con dati che divergono troppo da questa idea "effetto Wikipedia" di oggettività, magari ottenuti a caso da Internet, è di inquinare i modelli con centinaia di migliaia di opinioni assurde, minimamente argomentate, incapaci di resistere anche alla più semplice delle critiche e sostenute magari con violenza o in virtù dell'ignoranza.

**Cit. Vanolo(2023)***: Si tratta, in particolare, di provare a sonda-*

*re l'"inconscio politico" -- per citare la celebre espressione di Jameson (1981) -- alla*

*base dei suoi testi e delle sue risposte, spesso apparentemente semplici e lineari. La*

*premessa, piuttosto evidente, è che le tecnologie non sono mai medium neutrali,*

*ma rilettono sempre prospettive e posizionamenti, e in questo senso contengono*

*inevitabilmente una dimensione politica (Peters, 2022). Per formulare un esempio*

*molto semplice, se domandiamo a un sistema di IA o più semplicemente a Google*

*quale sia la capitale di Israele, una risposta apparentemente semplice e univoca*

*come «Gerusalemme» contiene chiaramente un posizionamento politico molto*

*1*

*forte .*

*Si utilizza qui l'espressione 'inconscio' , in maniera metaforica, per enfatizzarne*

*la natura sub-cosciente, sommersa e quasi invisibile, ma nondimeno palpabile e*

*rilevante. Gli algoritmi dell'IA sono infatti costituiti da reti neurali alimentate da*

*centinaia di gigabytes di informazioni digitali. Quando dialoghiamo con una IA,*

*siamo coscienti del fatto che 'ha letto' (il termine è volutamente improprio) milioni*

*di libri, articoli, ricerche scientiiche, ed è in grado di rielaborarle e riassumerle. Ci*

*si aspetta che sia in grado di fornire le risposte più accurate a domande molto spe-*

*ciiche e convenzionali, come la temperatura media annua di una località del Sene-*

*gal. Allo stesso tempo, nelle scienze sociali sappiamo bene come ogni conoscenza,*

*ogni discorso e ogni pratica di rappresentazione e riproduzione del sapere non*

*sia mai oggettiva, ma contenga punti di vista, prospettive, logiche di potere. Nel*

*caso dell'IA, è plausibile immaginare che la sua posizione riletta il mainstream*

*nel senso più semplice del termine: fornisce le risposte dominanti, avvalorate dalla*

*comunità scientiica, oltre -- come vedremo -- ad avere la tendenza ad assecondare*

*le domande di chi si trova davanti allo schermo.*

La questione della licenza verrà affrontata in seguito, prevista comunque una Creative Commons Attribuzione (eventualmente con aggiunta Non Commerciale e Non Derivati) ad uso "umano" per evitare plagi, ma con liberatoria per quanto riguarda l'addestramento dei modelli (pubblico dominio)

© 2023 Matteo Rinaldi
